{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count \n",
    "\n",
    "The word count program is the \"hello world\" of distributed computing, and is an essential to learning how to engage the cluster with Spark. \n",
    "\n",
    "When running Spark with the jupyter notebook driver, a Spark context is automatically added to the interpreter, accessible with the `sc` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f666dfb6250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load an RDD &mdash; a resilient distributed dataset &mdash; with data from HDFS. We'll load the complete works of Shakespeare, which is stored on HDFS under our user folder. \n",
    "\n",
    "The RDD is loaded as a collection of strings, where each element in the RDD is a single line from the text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = sc.textFile(\"hdfs://10.0.0.125/user/ec2-user/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count words, we need to split the string lines up by white space, we can use Python's `str.split()` for this. To apply transformations to the RDD, we will pass closures that describe the work, mapping them to each element of the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = text.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations _do not execute_ immediately after they are declared. Instead a _lineage_ is built up, describing interactions with RDDs as they are transformed into new RDDs. We can see the lineage by printing the DAG as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[2] at RDD at PythonRDD.scala:43 []\n",
      " |  hdfs://10.0.0.125/user/ec2-user/shakespeare.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      " |  hdfs://10.0.0.125/user/ec2-user/shakespeare.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print(text.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to cause execution to occur on the cluster, we need to call an _action_ method on the RDD. For example we can use the `take` method to collect the first element of the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'hamlet@0', u'HAMLET']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitter(sep=\" \"):\n",
    "    def inner(line):\n",
    "        return line.split(sep)\n",
    "    return inner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_splitter = splitter(\"\\t\")\n",
    "comma_splitter = splitter(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a\\tb\\tc'], ['a', 'b', 'c']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(comma_splitter, [\"a\\tb\\tc\", \"a,b,c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEP = \",\"\n",
    "\n",
    "def splitter2(line):\n",
    "    return line.split(SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a\\tb\\tc'], ['a', 'b', 'c']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(splitter2, [\"a\\tb\\tc\", \"a,b,c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "lens = text.map(lambda x: len(x))\n",
    "count = lens.reduce(add)\n",
    "print(count.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = sc.textFile(\"hdfs://10.0.0.125/user/ec2-user/shakespeare.txt\")\n",
    "text = text.flatMap(lambda line: line.split())\n",
    "text = text.map(lambda word: (word, 1))\n",
    "text = text.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = text.sortBy(lambda token: token[1], ascending=False)\n",
    "text.coalesce(1).saveAsTextFile('shakes-counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'HAMLET',\n",
       " u'DRAMATIS',\n",
       " u'PERSONAE',\n",
       " u'CLAUDIUS',\n",
       " u'king',\n",
       " u'of',\n",
       " u'Denmark.',\n",
       " u'(KING',\n",
       " u'CLAUDIUS:)',\n",
       " u'HAMLET',\n",
       " u'son',\n",
       " u'to',\n",
       " u'the',\n",
       " u'late,',\n",
       " u'and',\n",
       " u'nephew',\n",
       " u'to',\n",
       " u'the',\n",
       " u'present',\n",
       " u'king.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(\"hdfs://10.0.0.125/user/ec2-user/shakespeare.txt\")\n",
    "text = text.map(lambda line: \" \".join(line.split(\"\\t\")[1:]))\n",
    "text = text.flatMap(lambda lineno: lineno.split())\n",
    "\n",
    "text.take(20)\n",
    "\n",
    "# lens = text.map(lambda words: len(words))\n",
    "\n",
    "# lens.reduce(add)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
